{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af56118",
   "metadata": {},
   "source": [
    "# Tutorial 3: Performance Evaluation Framework\n",
    "\n",
    "This tutorial provides a deep dive into the **le_eval** module — the performance\n",
    "benchmarking framework for systematically measuring and comparing detector\n",
    "performance across image datasets.\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "1. **StringTable** — structured result storage and CSV export\n",
    "2. **Performance Primitives** — `PerformanceResult`, `PerformanceMeasureBase`,\n",
    "   `CVPerformanceMeasure`\n",
    "3. **Data Providers** — loading images from files for benchmarking\n",
    "4. **Writing Custom Tasks** — subclassing `CVPerformanceTask` in Python\n",
    "5. **CVPerformanceTest** — the orchestrator for running benchmarks\n",
    "6. **Result Analysis** — accumulating, visualizing, and exporting results\n",
    "7. **Full Benchmark** — complete end-to-end example\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed **Tutorial 1 & 2** (or equivalent knowledge of `le_imgproc`,\n",
    "  `le_edge`, `le_lsd`)\n",
    "- Built the LE Python bindings: `bazel build //libs/...`\n",
    "- Python kernel set to the project `.venv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb3beb",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib, time, tempfile, os\n",
    "\n",
    "# --- Locate workspace root and add Bazel output dirs to sys.path ---\n",
    "workspace = pathlib.Path.cwd()\n",
    "while not (workspace / \"MODULE.bazel\").exists():\n",
    "    if workspace == workspace.parent:\n",
    "        raise RuntimeError(\"Cannot find LineExtraction workspace root (MODULE.bazel)\")\n",
    "    workspace = workspace.parent\n",
    "\n",
    "for lib in [\"imgproc\", \"edge\", \"geometry\", \"eval\", \"lsd\"]:\n",
    "    p = workspace / f\"bazel-bin/libs/{lib}/python\"\n",
    "    if p.exists():\n",
    "        sys.path.insert(0, str(p))\n",
    "    else:\n",
    "        print(f\"Warning: Not found: {p}  — run: bazel build //libs/{lib}/...\")\n",
    "\n",
    "sys.path.insert(0, str(workspace / \"python\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NOTE: Do NOT import cv2 — the LE bindings ship their own statically linked\n",
    "# OpenCV build. Using pip's cv2 would cause symbol conflicts.\n",
    "\n",
    "import le_eval\n",
    "import le_imgproc\n",
    "import le_edge\n",
    "import le_lsd\n",
    "import le_geometry\n",
    "from lsfm.data import TestImages\n",
    "\n",
    "print(f\"Workspace: {workspace}\")\n",
    "print(f\"le_eval loaded: {len([x for x in dir(le_eval) if not x.startswith('_')])} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f33954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Common helpers ---\n",
    "\n",
    "def show_images(images, titles, cmap=\"gray\", figsize=None):\n",
    "    \"\"\"Show a list of images side by side.\"\"\"\n",
    "    n = len(images)\n",
    "    if figsize is None:\n",
    "        figsize = (4 * n, 4)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img, cmap=cmap if img.ndim == 2 else None)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cacb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. StringTable — Structured Result Storage\n",
    "\n",
    "`StringTable` is a simple 2D table of strings used for collecting and\n",
    "displaying benchmark results. It supports indexing, row/column extraction,\n",
    "transposition, and CSV export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction: StringTable(rows, cols)\n",
    "table = le_eval.StringTable(3, 4)\n",
    "print(f\"Empty table: {table.rows} rows × {table.cols} cols, size={table.size}\")\n",
    "\n",
    "# Set values via [row, col] indexing\n",
    "headers = [\"Detector\", \"Segments\", \"Avg Len\", \"Time (ms)\"]\n",
    "for c, h in enumerate(headers):\n",
    "    table[0, c] = h\n",
    "\n",
    "data = [\n",
    "    [\"LsdCC\", \"142\", \"23.5\", \"4.2\"],\n",
    "    [\"LsdFGioi\", \"98\", \"31.2\", \"8.7\"],\n",
    "]\n",
    "for r, row in enumerate(data):\n",
    "    for c, val in enumerate(row):\n",
    "        table[r + 1, c] = val\n",
    "\n",
    "print(f\"\\nTable contents:\")\n",
    "print(repr(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row and column extraction\n",
    "print(\"Header row:\", table.row(0))\n",
    "print(\"Detector column:\", table.col(0))\n",
    "\n",
    "# To list (full 2D)\n",
    "all_data = table.to_list()\n",
    "print(\"\\nFull data as list:\")\n",
    "for row in all_data:\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# Transpose\n",
    "t = table.transpose()\n",
    "print(f\"\\nTransposed: {t.rows}×{t.cols}\")\n",
    "print(\"First row of transposed:\", t.row(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "csv_path = os.path.join(tempfile.gettempdir(), \"le_results.csv\")\n",
    "table.save_csv(csv_path)\n",
    "\n",
    "with open(csv_path) as f:\n",
    "    print(f\"CSV content ({csv_path}):\")\n",
    "    print(f.read())\n",
    "\n",
    "os.unlink(csv_path)  # clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7be14",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "Create a `StringTable` with 5 rows and 3 columns. Fill the header row with\n",
    "`[\"Name\", \"Value\", \"Unit\"]`. Add 4 data rows describing different image\n",
    "properties (e.g., width, height, channels, dtype). Print it, transpose it,\n",
    "and save the transposed version to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b43025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a StringTable, fill it, transpose, save to CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b4686",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = le_eval.StringTable(5, 3)\n",
    "t[0, 0] = \"Name\"; t[0, 1] = \"Value\"; t[0, 2] = \"Unit\"\n",
    "t[1, 0] = \"Width\";    t[1, 1] = \"1920\";  t[1, 2] = \"px\"\n",
    "t[2, 0] = \"Height\";   t[2, 1] = \"1080\";  t[2, 2] = \"px\"\n",
    "t[3, 0] = \"Channels\"; t[3, 1] = \"3\";     t[3, 2] = \"-\"\n",
    "t[4, 0] = \"Dtype\";    t[4, 1] = \"uint8\"; t[4, 2] = \"-\"\n",
    "\n",
    "print(repr(t))\n",
    "print()\n",
    "\n",
    "tt = t.transpose()\n",
    "print(f\"Transposed ({tt.rows}×{tt.cols}):\")\n",
    "print(repr(tt))\n",
    "\n",
    "csv_path = os.path.join(tempfile.gettempdir(), \"image_props.csv\")\n",
    "tt.save_csv(csv_path)\n",
    "with open(csv_path) as f:\n",
    "    print(f.read())\n",
    "os.unlink(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111e976",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Performance Primitives\n",
    "\n",
    "The evaluation framework uses several types for collecting and computing\n",
    "performance statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f60704",
   "metadata": {},
   "source": [
    "### 3.1 PerformanceResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e00b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerformanceResult holds computed statistics: total, mean, stddev\n",
    "result = le_eval.PerformanceResult()\n",
    "print(f\"Default: total={result.total}, mean={result.mean}, stddev={result.stddev}\")\n",
    "\n",
    "# Set values directly\n",
    "result.total = 150.0\n",
    "result.mean = 15.0\n",
    "result.stddev = 2.3\n",
    "print(f\"Set:     total={result.total}, mean={result.mean}, stddev={result.stddev}\")\n",
    "print(repr(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40183de0",
   "metadata": {},
   "source": [
    "### 3.2 PerformanceMeasureBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerformanceMeasureBase collects duration measurements and computes statistics.\n",
    "# Durations are in nanoseconds.\n",
    "\n",
    "pm = le_eval.PerformanceMeasureBase(\"windmill\", \"SobelGradient\")\n",
    "print(f\"source_name: '{pm.source_name}', task_name: '{pm.task_name}'\")\n",
    "print(f\"Initial durations: {pm.durations}\")\n",
    "\n",
    "# Simulate 10 timing measurements (in nanoseconds)\n",
    "simulated_ns = [1_200_000, 1_350_000, 1_100_000, 1_500_000, 1_250_000,\n",
    "                1_300_000, 1_400_000, 1_150_000, 1_280_000, 1_320_000]\n",
    "\n",
    "for ns in simulated_ns:\n",
    "    pm.append_duration(ns)\n",
    "\n",
    "print(f\"Durations: {len(pm.durations)} samples\")\n",
    "\n",
    "# Compute statistics\n",
    "result = pm.compute_result()\n",
    "print(f\"\\nResult:\")\n",
    "print(f\"  Total:  {result.total:.0f} ns ({result.total / 1e6:.2f} ms)\")\n",
    "print(f\"  Mean:   {result.mean:.0f} ns ({result.mean / 1e6:.2f} ms)\")\n",
    "print(f\"  Stddev: {result.stddev:.0f} ns ({result.stddev / 1e6:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear resets all collected data\n",
    "pm.clear()\n",
    "print(f\"After clear(): {len(pm.durations)} durations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0fa9b",
   "metadata": {},
   "source": [
    "### 3.3 CVPerformanceMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVPerformanceMeasure extends PerformanceMeasureBase with image dimensions.\n",
    "# This lets you compute throughput in megapixels/second.\n",
    "\n",
    "m = le_eval.CVPerformanceMeasure(\"windmill\", \"LsdCC\", 1920.0, 1080.0)\n",
    "print(f\"Dimensions: {m.width:.0f} × {m.height:.0f}\")\n",
    "print(f\"Megapixels: {m.mega_pixels():.2f}\")\n",
    "\n",
    "# Inherit duration tracking from PerformanceMeasureBase\n",
    "m.append_duration(5_000_000)  # 5 ms\n",
    "m.append_duration(4_800_000)\n",
    "m.append_duration(5_200_000)\n",
    "\n",
    "result = m.compute_result()\n",
    "print(f\"\\nMean time: {result.mean / 1e6:.2f} ms\")\n",
    "print(f\"Throughput: {m.mega_pixels() / (result.mean / 1e9):.1f} MP/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51cc39",
   "metadata": {},
   "source": [
    "### 3.4 Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure is a lightweight struct for labeling source/task pairs.\n",
    "m = le_eval.Measure()\n",
    "m.source_name = \"bsds_001\"\n",
    "m.task_name = \"LsdFGioi\"\n",
    "print(f\"source='{m.source_name}', task='{m.task_name}'\")\n",
    "\n",
    "m.clear()\n",
    "print(f\"After clear: source='{m.source_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc268a",
   "metadata": {},
   "source": [
    "### 3.5 GenericInputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530aa35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GenericInputData is a named container for input data.\n",
    "d = le_eval.GenericInputData(\"test_input\")\n",
    "print(f\"Name: '{d.name}'\")\n",
    "\n",
    "d.name = \"renamed\"\n",
    "print(f\"Renamed: '{d.name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efddb56",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Providers\n",
    "\n",
    "Data providers supply images to the benchmarking framework. They iterate\n",
    "over image files and wrap them in `CVData` or `CVPerformanceData` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b54932",
   "metadata": {},
   "source": [
    "### 4.1 CVData and CVPerformanceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d4582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVData wraps an image with a name\n",
    "d = le_eval.CVData()\n",
    "d.name = \"windmill\"\n",
    "d.src = np.zeros((100, 100, 3), dtype=np.uint8)  # Simulated image data\n",
    "\n",
    "print(f\"CVData: name='{d.name}', src shape={d.src.shape}\")\n",
    "\n",
    "# CVPerformanceData extends with metadata for benchmarking\n",
    "ti = TestImages()\n",
    "img = plt.imread(str(ti.windmill())).copy()  # .copy() ensures writable array\n",
    "pd = le_eval.CVPerformanceData(\"windmill\", img)\n",
    "print(f\"CVPerformanceData: name='{pd.name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207b885",
   "metadata": {},
   "source": [
    "### 4.2 File-Based Data Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileCVDataProvider loads images from a directory.\n",
    "provider = le_eval.FileCVDataProvider(\"test_provider\")\n",
    "print(f\"Provider name: '{provider.name}'\")\n",
    "\n",
    "# rewind() and clear() manage the iterator state\n",
    "provider.rewind()\n",
    "provider.clear()\n",
    "print(\"Provider reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileCVPerformanceDataProvider is the performance-oriented variant\n",
    "perf_provider = le_eval.FileCVPerformanceDataProvider(\"perf_provider\")\n",
    "print(f\"Perf provider: '{perf_provider.name}'\")\n",
    "perf_provider.rewind()\n",
    "perf_provider.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647646b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileDataProvider is a convenience wrapper that takes a directory path\n",
    "with tempfile.TemporaryDirectory() as data_dir:\n",
    "    fp = le_eval.FileDataProvider(data_dir, \"temp_data\")\n",
    "    print(f\"FileDataProvider: name='{fp.name}', dir='{data_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b47ee",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Writing Custom Tasks\n",
    "\n",
    "The evaluation framework lets you wrap any algorithm as a **task** that\n",
    "can be benchmarked. There are two main approaches:\n",
    "\n",
    "1. Subclass `Task` for generic tasks\n",
    "2. Subclass `CVPerformanceTask` for image processing benchmarks\n",
    "\n",
    "### Task Flags\n",
    "\n",
    "| Flag | Value | Meaning |\n",
    "|------|-------|---------|\n",
    "| `TASK_SQR` | 1 | Convert input to grayscale |\n",
    "| `TASK_RGB` | 2 | Keep input as RGB |\n",
    "| `TASK_NO_3` | 4 | Disable 3-channel processing |\n",
    "| `TASK_NO_5` | 8 | Disable 5-channel processing |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edccb3c",
   "metadata": {},
   "source": [
    "### 5.1 Basic Task Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5021404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Task requires overriding run(loops)\n",
    "class CountingTask(le_eval.Task):\n",
    "    \"\"\"Simple task that counts invocations.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        self.call_count = 0\n",
    "    \n",
    "    def run(self, loops: int) -> None:\n",
    "        self.call_count += loops\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.call_count = 0\n",
    "\n",
    "task = CountingTask(\"counter\")\n",
    "print(f\"Task name: '{task.name}', task_name(): '{task.task_name()}'\")\n",
    "print(f\"Verbose: {task.verbose}\")\n",
    "\n",
    "# Verify it's an ITask\n",
    "print(f\"Is ITask: {isinstance(task, le_eval.ITask)}\")\n",
    "\n",
    "task.run(5)\n",
    "task.run(3)\n",
    "print(f\"Call count: {task.call_count}\")\n",
    "\n",
    "task.reset()\n",
    "print(f\"After reset: {task.call_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task also supports save_visual_results() override\n",
    "class SavingTask(le_eval.Task):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        self.saved_path = None\n",
    "    \n",
    "    def run(self, loops: int) -> None:\n",
    "        pass\n",
    "    \n",
    "    def save_visual_results(self, target_path: str) -> None:\n",
    "        self.saved_path = target_path\n",
    "        print(f\"Would save results to: {target_path}\")\n",
    "\n",
    "st = SavingTask(\"saver\")\n",
    "st.save_visual_results(\"/tmp/results\")\n",
    "print(f\"Saved path: {st.saved_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76ef23",
   "metadata": {},
   "source": [
    "### 5.2 CVPerformanceTask — Image Processing Benchmarks\n",
    "\n",
    "`CVPerformanceTask` is the right base class for benchmarking image processing\n",
    "algorithms. Override:\n",
    "\n",
    "- `prepare_impl(src)` — one-time setup before timing (optional)\n",
    "- `run_impl(name, src)` — the timed operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ee456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CVPerformanceTask: wraps a gradient filter\n",
    "class GradientTask(le_eval.CVPerformanceTask):\n",
    "    \"\"\"Benchmark task for gradient computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, gradient_cls) -> None:\n",
    "        # TASK_SQR flag = convert input to grayscale\n",
    "        super().__init__(name, le_eval.TASK_SQR)\n",
    "        self.gradient_cls = gradient_cls\n",
    "        self.grad = None\n",
    "    \n",
    "    def prepare_impl(self, src: np.ndarray) -> None:\n",
    "        # Create a fresh gradient filter for each image\n",
    "        self.grad = self.gradient_cls()\n",
    "    \n",
    "    def run_impl(self, name: str, src: np.ndarray) -> None:\n",
    "        self.grad.process(src)\n",
    "\n",
    "# Test construction\n",
    "gt = GradientTask(\"Sobel\", le_imgproc.SobelGradient)\n",
    "print(f\"Task name: '{gt.name}'\")\n",
    "print(f\"sqr(): {gt.sqr()}, rgb(): {gt.rgb()}\")\n",
    "print(f\"border(): {gt.border()}\")\n",
    "print(f\"verbose: {gt.verbose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aeed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task flags control input preprocessing\n",
    "print(\"Task flag constants:\")\n",
    "print(f\"  TASK_SQR  = {le_eval.TASK_SQR}  (convert to grayscale)\")\n",
    "print(f\"  TASK_RGB  = {le_eval.TASK_RGB}  (keep as RGB)\")\n",
    "print(f\"  TASK_NO_3 = {le_eval.TASK_NO_3}  (disable 3-channel)\")\n",
    "print(f\"  TASK_NO_5 = {le_eval.TASK_NO_5}  (disable 5-channel)\")\n",
    "\n",
    "# Combining flags\n",
    "combined = le_eval.CVPerformanceTask(\"combined\", le_eval.TASK_SQR | le_eval.TASK_NO_5)\n",
    "print(f\"\\nCombined flags: sqr={combined.sqr()}, rgb={combined.rgb()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complete task: wrap an LSD detector\n",
    "class LsdTask(le_eval.CVPerformanceTask):\n",
    "    \"\"\"Benchmark task for line segment detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, detector_cls, **kwargs) -> None:\n",
    "        super().__init__(name, le_eval.TASK_SQR)\n",
    "        self.detector_cls = detector_cls\n",
    "        self.kwargs = kwargs\n",
    "        self.det = None\n",
    "        self.last_count = 0\n",
    "    \n",
    "    def prepare_impl(self, src: np.ndarray) -> None:\n",
    "        self.det = self.detector_cls(**self.kwargs)\n",
    "    \n",
    "    def run_impl(self, name: str, src: np.ndarray) -> None:\n",
    "        self.det.detect(src)\n",
    "        self.last_count = len(self.det.line_segments())\n",
    "\n",
    "# Create tasks for several detectors\n",
    "tasks = [\n",
    "    LsdTask(\"LsdCC\", le_lsd.LsdCC),\n",
    "    LsdTask(\"LsdFGioi\", le_lsd.LsdFGioi),\n",
    "    LsdTask(\"LsdEDLZ\", le_lsd.LsdEDLZ),\n",
    "    LsdTask(\"LsdHoughP\", le_lsd.LsdHoughP),\n",
    "]\n",
    "\n",
    "for t in tasks:\n",
    "    print(f\"Task: '{t.name}', sqr={t.sqr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a15a60",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "Create a `CVPerformanceTask` subclass that benchmarks edge segment detection.\n",
    "It should:\n",
    "1. In `prepare_impl`, create an `EdgeSourceSobel` and an `EsdDrawing`\n",
    "2. In `run_impl`, process the image with the edge source and detect segments\n",
    "3. Store the number of detected segments in an instance variable\n",
    "\n",
    "Test it manually by calling `prepare_impl` and `run_impl` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b65051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an EsdTask subclass and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ac423",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88af4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsdTask(le_eval.CVPerformanceTask):\n",
    "    \"\"\"Benchmark edge segment detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, esd_cls, **kwargs) -> None:\n",
    "        super().__init__(name, le_eval.TASK_SQR)\n",
    "        self.esd_cls = esd_cls\n",
    "        self.kwargs = kwargs\n",
    "        self.es = None\n",
    "        self.esd = None\n",
    "        self.segment_count = 0\n",
    "    \n",
    "    def prepare_impl(self, src: np.ndarray) -> None:\n",
    "        self.es = le_edge.EdgeSourceSobel()\n",
    "        self.esd = self.esd_cls(**self.kwargs)\n",
    "    \n",
    "    def run_impl(self, name: str, src: np.ndarray) -> None:\n",
    "        self.es.process(src)\n",
    "        self.esd.detect(self.es)\n",
    "        self.segment_count = len(self.esd.segments())\n",
    "\n",
    "# Manual test\n",
    "task = EsdTask(\"EsdDrawing\", le_edge.EsdDrawing, min_pixels=5)\n",
    "\n",
    "# Load a test image\n",
    "ti = TestImages()\n",
    "img = plt.imread(str(ti.windmill())).copy()\n",
    "if img.ndim == 3:\n",
    "    gray = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140]).astype(np.uint8)\n",
    "else:\n",
    "    gray = img.copy()\n",
    "\n",
    "task.prepare_impl(gray)\n",
    "task.run_impl(\"windmill\", gray)\n",
    "print(f\"EsdTask detected {task.segment_count} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e09d7",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. CVPerformanceTest — The Orchestrator\n",
    "\n",
    "`CVPerformanceTest` ties everything together: it runs multiple tasks across\n",
    "multiple data providers and collects timing results into a `StringTable`.\n",
    "\n",
    "**Constructor:** `CVPerformanceTest(providers, name)`\n",
    "\n",
    "**Methods:**\n",
    "- `.add_task(task)` — register a task\n",
    "- `.result_table()` → `StringTable` with timing results\n",
    "- `.clear()` — reset all results\n",
    "\n",
    "**Display toggles:**\n",
    "- `.show_total` — include total time\n",
    "- `.show_mean` — include mean time (default: True)\n",
    "- `.show_std_dev` — include standard deviation\n",
    "- `.show_mega_pixel` — include megapixel throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41722927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic orchestrator setup\n",
    "providers = []  # Empty for now\n",
    "bench = le_eval.CVPerformanceTest(providers, \"demo_benchmark\")\n",
    "\n",
    "print(f\"show_total: {bench.show_total}\")\n",
    "print(f\"show_mean: {bench.show_mean}\")\n",
    "print(f\"show_std_dev: {bench.show_std_dev}\")\n",
    "print(f\"show_mega_pixel: {bench.show_mega_pixel}\")\n",
    "\n",
    "# Toggle display options\n",
    "bench.show_total = True\n",
    "bench.show_std_dev = True\n",
    "bench.show_mega_pixel = True\n",
    "\n",
    "# Add tasks\n",
    "bench.add_task(le_eval.CVPerformanceTask(\"dummy_task\", 0))\n",
    "\n",
    "# Get result table (empty since no data was processed)\n",
    "table = bench.result_table()\n",
    "print(f\"\\nResult table: {table.rows}×{table.cols}\")\n",
    "print(repr(table))\n",
    "\n",
    "bench.clear()\n",
    "print(\"\\nBenchmark cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9938822",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Accumulating Measures\n",
    "\n",
    "The `accumulate_measures()` function combines multiple `CVPerformanceMeasure`\n",
    "objects — useful for aggregating results across multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create measures for different images\n",
    "measures = []\n",
    "for i, (w, h) in enumerate([(640, 480), (1920, 1080), (800, 600)]):\n",
    "    m = le_eval.CVPerformanceMeasure(f\"image_{i}\", \"LsdCC\", float(w), float(h))\n",
    "    # Add some simulated durations\n",
    "    for _ in range(5):\n",
    "        duration = int((w * h * 0.01 + np.random.normal(0, 100)) * 1000)  # ns\n",
    "        m.append_duration(max(1, duration))\n",
    "    r = m.compute_result()\n",
    "    measures.append(m)\n",
    "    print(f\"Image {i}: {w}×{h} = {m.mega_pixels():.2f} MP, \"\n",
    "          f\"mean={r.mean/1e6:.2f} ms\")\n",
    "\n",
    "# Accumulate\n",
    "accumulated = le_eval.accumulate_measures(measures)\n",
    "print(f\"\\nAccumulated: {len(accumulated.durations)} durations, \"\n",
    "      f\"{accumulated.mega_pixels():.2f} MP\")\n",
    "\n",
    "acc_result = accumulated.compute_result()\n",
    "print(f\"Accumulated mean: {acc_result.mean/1e6:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulating empty list\n",
    "empty_acc = le_eval.accumulate_measures([])\n",
    "print(f\"Empty accumulation: {empty_acc.mega_pixels():.2f} MP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee90da3",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Full Benchmark Example\n",
    "\n",
    "Let's put everything together and run a complete benchmark comparing\n",
    "multiple LSD detectors across several images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "import cv2\n",
    "\n",
    "ti = TestImages()\n",
    "\n",
    "# Windmill\n",
    "wm = cv2.imread(str(ti.windmill()), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# A few BSDS500 images\n",
    "bsds_iter = ti.bsds500()\n",
    "bsds_images = []\n",
    "for _ in range(3):\n",
    "    try:\n",
    "        path = next(bsds_iter)\n",
    "        img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "        bsds_images.append((path.stem, img))\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "# Collect all test images\n",
    "test_images = [(\"windmill\", wm)] + bsds_images\n",
    "print(f\"Loaded {len(test_images)} test images:\")\n",
    "for name, img in test_images:\n",
    "    print(f\"  {name}: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detectors to benchmark\n",
    "detector_config = [\n",
    "    (\"LsdCC\", le_lsd.LsdCC, {}),\n",
    "    (\"LsdCP\", le_lsd.LsdCP, {}),\n",
    "    (\"LsdBurns\", le_lsd.LsdBurns, {}),\n",
    "    (\"LsdFBW\", le_lsd.LsdFBW, {}),\n",
    "    (\"LsdFGioi\", le_lsd.LsdFGioi, {}),\n",
    "    (\"LsdEDLZ\", le_lsd.LsdEDLZ, {}),\n",
    "    (\"LsdEL\", le_lsd.LsdEL, {}),\n",
    "    (\"LsdEP\", le_lsd.LsdEP, {}),\n",
    "    (\"LsdHoughP\", le_lsd.LsdHoughP, {}),\n",
    "]\n",
    "\n",
    "N_LOOPS = 3\n",
    "\n",
    "# Run benchmark\n",
    "results = {}  # {detector_name: {image_name: (n_segments, mean_time_ms)}}\n",
    "\n",
    "for det_name, det_cls, kwargs in detector_config:\n",
    "    results[det_name] = {}\n",
    "    for img_name, img in test_images:\n",
    "        det = det_cls(**kwargs)\n",
    "        \n",
    "        # Warm up\n",
    "        det.detect(img)\n",
    "        n_segs = len(det.line_segments())\n",
    "        \n",
    "        # Time it\n",
    "        times = []\n",
    "        for _ in range(N_LOOPS):\n",
    "            t0 = time.perf_counter_ns()\n",
    "            det.detect(img)\n",
    "            t1 = time.perf_counter_ns()\n",
    "            times.append((t1 - t0) / 1e6)  # ms\n",
    "        \n",
    "        mean_ms = np.mean(times)\n",
    "        results[det_name][img_name] = (n_segs, mean_ms)\n",
    "\n",
    "print(\"Benchmark complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a StringTable from results\n",
    "img_names = [name for name, _ in test_images]\n",
    "det_names = [name for name, _, _ in detector_config]\n",
    "\n",
    "# Table: segments per detector per image\n",
    "n_rows = len(det_names) + 1\n",
    "n_cols = len(img_names) + 1\n",
    "\n",
    "seg_table = le_eval.StringTable(n_rows, n_cols)\n",
    "seg_table[0, 0] = \"Detector\"\n",
    "for c, iname in enumerate(img_names):\n",
    "    seg_table[0, c + 1] = iname\n",
    "\n",
    "for r, dname in enumerate(det_names):\n",
    "    seg_table[r + 1, 0] = dname\n",
    "    for c, iname in enumerate(img_names):\n",
    "        n_segs, _ = results[dname][iname]\n",
    "        seg_table[r + 1, c + 1] = str(n_segs)\n",
    "\n",
    "print(\"Segments detected:\")\n",
    "print(repr(seg_table))\n",
    "\n",
    "# Time table\n",
    "time_table = le_eval.StringTable(n_rows, n_cols)\n",
    "time_table[0, 0] = \"Detector\"\n",
    "for c, iname in enumerate(img_names):\n",
    "    time_table[0, c + 1] = iname\n",
    "\n",
    "for r, dname in enumerate(det_names):\n",
    "    time_table[r + 1, 0] = dname\n",
    "    for c, iname in enumerate(img_names):\n",
    "        _, ms = results[dname][iname]\n",
    "        time_table[r + 1, c + 1] = f\"{ms:.2f}\"\n",
    "\n",
    "print(\"\\nDetection time (ms):\")\n",
    "print(repr(time_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ffa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "csv_seg = os.path.join(tempfile.gettempdir(), \"le_segments.csv\")\n",
    "csv_time = os.path.join(tempfile.gettempdir(), \"le_times.csv\")\n",
    "\n",
    "seg_table.save_csv(csv_seg)\n",
    "time_table.save_csv(csv_time)\n",
    "\n",
    "print(f\"Saved: {csv_seg}\")\n",
    "print(f\"Saved: {csv_time}\")\n",
    "\n",
    "# Show CSV content\n",
    "with open(csv_time) as f:\n",
    "    print(f\"\\nTime CSV:\\n{f.read()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a48386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: bar chart per image\n",
    "fig, axes = plt.subplots(1, len(img_names), figsize=(6 * len(img_names), 5))\n",
    "if len(img_names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, iname in zip(axes, img_names):\n",
    "    segs = [results[d][iname][0] for d in det_names]\n",
    "    ax.barh(det_names, segs, color=\"steelblue\")\n",
    "    ax.set_xlabel(\"Segments\")\n",
    "    ax.set_title(f\"{iname}\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.suptitle(\"Segments Detected per Image\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3decbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: timing comparison (average across all images)\n",
    "avg_times = []\n",
    "avg_segs = []\n",
    "for dname in det_names:\n",
    "    times = [results[dname][iname][1] for iname in img_names]\n",
    "    segs = [results[dname][iname][0] for iname in img_names]\n",
    "    avg_times.append(np.mean(times))\n",
    "    avg_segs.append(np.mean(segs))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.barh(det_names, avg_times, color=\"coral\")\n",
    "ax1.set_xlabel(\"Mean Time (ms)\")\n",
    "ax1.set_title(\"Average Detection Time\")\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.barh(det_names, avg_segs, color=\"seagreen\")\n",
    "ax2.set_xlabel(\"Mean Segments\")\n",
    "ax2.set_title(\"Average Segments Detected\")\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01088590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: time vs segments (speed vs quantity trade-off)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, dname in enumerate(det_names):\n",
    "    plt.scatter(avg_times[i], avg_segs[i], s=100, zorder=5)\n",
    "    plt.annotate(dname, (avg_times[i], avg_segs[i]),\n",
    "                 textcoords=\"offset points\", xytext=(5, 5), fontsize=9)\n",
    "\n",
    "plt.xlabel(\"Mean Detection Time (ms)\")\n",
    "plt.ylabel(\"Mean Segments Detected\")\n",
    "plt.title(\"Speed vs. Quantity Trade-Off\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c2980",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "Extend the benchmark:\n",
    "\n",
    "1. Create a `CVPerformanceMeasure` for each (detector, image) pair\n",
    "2. Accumulate the measures per detector using `accumulate_measures()`\n",
    "3. Compute throughput in megapixels/second for each detector\n",
    "4. Create a bar chart showing MP/s for each detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd60917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build CVPerformanceMeasures, accumulate, compute throughput, plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac280b1",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4156186",
   "metadata": {},
   "outputs": [],
   "source": [
    "throughputs = []\n",
    "\n",
    "for det_name, det_cls, kwargs in detector_config:\n",
    "    measures = []\n",
    "    for img_name, img in test_images:\n",
    "        h, w = img.shape[:2]\n",
    "        m = le_eval.CVPerformanceMeasure(img_name, det_name, float(w), float(h))\n",
    "        \n",
    "        det = det_cls(**kwargs)\n",
    "        for _ in range(N_LOOPS):\n",
    "            t0 = time.perf_counter_ns()\n",
    "            det.detect(img)\n",
    "            t1 = time.perf_counter_ns()\n",
    "            m.append_duration(t1 - t0)\n",
    "        \n",
    "        measures.append(m)\n",
    "    \n",
    "    acc = le_eval.accumulate_measures(measures)\n",
    "    r = acc.compute_result()\n",
    "    mp_per_sec = acc.mega_pixels() / (r.mean / 1e9) if r.mean > 0 else 0\n",
    "    throughputs.append(mp_per_sec)\n",
    "    print(f\"{det_name:12s}: {mp_per_sec:.1f} MP/s (mean: {r.mean/1e6:.2f} ms)\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(det_names, throughputs, color=\"teal\")\n",
    "plt.xlabel(\"Throughput (Megapixels / second)\")\n",
    "plt.title(\"Detector Throughput\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c679a23",
   "metadata": {},
   "source": [
    "Throughput normalizes for image size, making it possible to fairly compare\n",
    "performance across images of different resolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d7395",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "Benchmark the **noise robustness** of `LsdEL`:\n",
    "\n",
    "1. Load the bike image and its noise variants from `TestImages.noise()`\n",
    "2. Run `LsdEL` on each, collecting timing with `CVPerformanceMeasure`\n",
    "3. Plot noise level vs. (a) detection time and (b) segments detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load noise images, benchmark LsdEL on each, plot results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b6709",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = TestImages()\n",
    "\n",
    "noise_names = [\"bike\", \"bike_noise10\", \"bike_noise20\", \"bike_noise30\", \"bike_noise50\"]\n",
    "noise_images = {}\n",
    "for name in noise_names:\n",
    "    img = cv2.imread(str(ti.noise(f\"{name}.png\")), cv2.IMREAD_GRAYSCALE)\n",
    "    noise_images[name] = img\n",
    "\n",
    "# Benchmark\n",
    "noise_times = []\n",
    "noise_segs = []\n",
    "\n",
    "for name, img in noise_images.items():\n",
    "    h, w = img.shape[:2]\n",
    "    m = le_eval.CVPerformanceMeasure(name, \"LsdEL\", float(w), float(h))\n",
    "    \n",
    "    det = le_lsd.LsdEL()\n",
    "    for _ in range(5):\n",
    "        t0 = time.perf_counter_ns()\n",
    "        det.detect(img)\n",
    "        t1 = time.perf_counter_ns()\n",
    "        m.append_duration(t1 - t0)\n",
    "    \n",
    "    r = m.compute_result()\n",
    "    n = len(det.line_segments())\n",
    "    noise_times.append(r.mean / 1e6)\n",
    "    noise_segs.append(n)\n",
    "    print(f\"{name:16s}: {n:4d} segs, {r.mean/1e6:.2f} ms\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(noise_names, noise_times, \"o-\", color=\"coral\")\n",
    "ax1.set_ylabel(\"Time (ms)\")\n",
    "ax1.set_title(\"Detection Time vs. Noise\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "ax2.plot(noise_names, noise_segs, \"o-\", color=\"steelblue\")\n",
    "ax2.set_ylabel(\"Segments\")\n",
    "ax2.set_title(\"Segments vs. Noise\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a47858",
   "metadata": {},
   "source": [
    "Noise typically increases both detection time (more candidate edges to process)\n",
    "and segment count (spurious short detections). At very high noise, some detectors\n",
    "may actually slow down due to the larger number of seed points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e90e28",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial you learned:\n",
    "\n",
    "- **StringTable**: Structured result storage with CSV export for reproducible\n",
    "  reporting\n",
    "- **Performance Primitives**: `PerformanceResult`, `PerformanceMeasureBase`, and\n",
    "  `CVPerformanceMeasure` for collecting and computing timing statistics\n",
    "- **Data Providers**: File-based image loading for systematic benchmarking\n",
    "- **Custom Tasks**: Wrapping any algorithm as a `CVPerformanceTask` for\n",
    "  integration with the benchmarking framework\n",
    "- **CVPerformanceTest**: The orchestrator for running multi-task, multi-image\n",
    "  benchmarks\n",
    "- **Result Analysis**: Accumulating measures across images, computing throughput,\n",
    "  and creating publication-quality comparison charts\n",
    "\n",
    "### Full Tutorial Series\n",
    "\n",
    "- **CV Primer** — Image processing fundamentals for beginners\n",
    "- **Tutorial 1** — Library fundamentals (`le_imgproc`, `le_geometry`, `TestImages`)\n",
    "- **Tutorial 2** — Edge & line detection pipelines (`le_edge`, `le_lsd`)\n",
    "- **Tutorial 3** — Performance evaluation framework (`le_eval`) ← you are here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
