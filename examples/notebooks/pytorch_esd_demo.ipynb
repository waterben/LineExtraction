{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db57517",
   "metadata": {},
   "source": [
    "# PyTorch Object Segmentation + ESD Line Extraction Demo\n",
    "\n",
    "This notebook demonstrates the integration of **PyTorch-based object segmentation** with the\n",
    "**LineExtraction ESD (Edge Segment Detector)** framework.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────┐    ┌──────────────────────┐    ┌─────────────────────┐    ┌────────────────┐\n",
    "│  Input Image        │───>│ PyTorch Segmentation │───>│  Extract Contours   │───>│  ESD + Lines   │\n",
    "│  (BSDS500/MDB)      │    │  (SAM / YOLO)        │    │  (cv2.findContours) │    │  (le_edge/lsd) │\n",
    "└─────────────────────┘    └──────────────────────┘    └─────────────────────┘    └────────────────┘\n",
    "```\n",
    "\n",
    "**Interactive Features:**\n",
    "- **SAM (Segment Anything):** Click on any object to segment it\n",
    "- **YOLO:** Automatic instance segmentation of 80 COCO classes\n",
    "- **Model switching:** Toggle between SAM and YOLO in real-time\n",
    "- **ESD integration:** Convert object contours to line segments\n",
    "\n",
    "**Prerequisites:**\n",
    "```bash\n",
    "# Build Python bindings\n",
    "bazel build //libs/...\n",
    "\n",
    "# Install PyTorch dependencies\n",
    "uv pip install torch torchvision segment-anything ultralytics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e20d5c6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Configure the Python environment to use LineExtraction bindings from Bazel output directories.\n",
    "Import PyTorch, segmentation models, and LE modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b393e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Enable interactive matplotlib backend for click events\n",
    "%matplotlib widget\n",
    "\n",
    "# --- Locate workspace root and add Bazel output dirs to sys.path ---\n",
    "workspace = pathlib.Path.cwd()\n",
    "while not (workspace / \"MODULE.bazel\").exists():\n",
    "    if workspace == workspace.parent:\n",
    "        raise RuntimeError(\"Cannot find LineExtraction workspace root (MODULE.bazel)\")\n",
    "    workspace = workspace.parent\n",
    "\n",
    "# Add each binding's Bazel output directory\n",
    "for lib in [\"imgproc\", \"edge\", \"geometry\", \"eval\", \"lsd\"]:\n",
    "    p = workspace / f\"bazel-bin/libs/{lib}/python\"\n",
    "    if p.exists():\n",
    "        sys.path.insert(0, str(p))\n",
    "    else:\n",
    "        print(f\"⚠ Not found: {p}  — run: bazel build //libs/{lib}/...\")\n",
    "\n",
    "# Add lsfm package for TestImages\n",
    "sys.path.insert(0, str(workspace / \"python\"))\n",
    "\n",
    "print(f\"Workspace: {workspace}\")\n",
    "\n",
    "# Import LineExtraction modules\n",
    "import le_imgproc\n",
    "import le_edge\n",
    "import le_geometry\n",
    "import le_lsd\n",
    "\n",
    "# Import test images helper\n",
    "from lsfm.data import TestImages\n",
    "\n",
    "# Check for PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"PyTorch: {torch.__version__}, CUDA available: {torch.cuda.is_available()}\")\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except ImportError:\n",
    "    raise ImportError(\"PyTorch not installed. Run: uv pip install torch torchvision\")\n",
    "\n",
    "# Check for segmentation models\n",
    "SAM_AVAILABLE = False\n",
    "YOLO_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    SAM_AVAILABLE = True\n",
    "    print(\"✓ Segment Anything Model (SAM) available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ SAM not installed. Run: uv pip install segment-anything\")\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    YOLO_AVAILABLE = True\n",
    "    print(\"✓ Ultralytics YOLO available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ YOLO not installed. Run: uv pip install ultralytics\")\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"\\nAll modules loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6e69d",
   "metadata": {},
   "source": [
    "## 2. Model Abstraction Layer\n",
    "\n",
    "Define a common interface for segmentation models, enabling seamless switching between SAM and YOLO.\n",
    "Each model must implement:\n",
    "- `load_model()` — Download and initialize the model\n",
    "- `predict_mask(image, point)` — Generate binary mask from input\n",
    "- `get_contours(mask)` — Extract polygon contours from mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SegmentationResult:\n",
    "    \"\"\"Result from a segmentation model.\"\"\"\n",
    "    masks: List[np.ndarray]  # List of binary masks (H, W), one per detected object\n",
    "    contours: List[np.ndarray]  # List of contour arrays, each shape (N, 2) as (x, y)\n",
    "    labels: List[str]  # Class labels for each mask\n",
    "    scores: List[float]  # Confidence scores\n",
    "\n",
    "\n",
    "class SegmentationModel(ABC):\n",
    "    \"\"\"Abstract base class for segmentation models.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self._loaded = False\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"Load the model weights. Called lazily on first prediction.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(\n",
    "        self, \n",
    "        image: np.ndarray, \n",
    "        point: Optional[Tuple[int, int]] = None\n",
    "    ) -> SegmentationResult:\n",
    "        \"\"\"\n",
    "        Predict segmentation masks for the image.\n",
    "        \n",
    "        Args:\n",
    "            image: RGB image as numpy array (H, W, 3), uint8\n",
    "            point: Optional (x, y) click coordinate for interactive segmentation (SAM)\n",
    "        \n",
    "        Returns:\n",
    "            SegmentationResult with masks, contours, labels, and scores\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def ensure_loaded(self) -> None:\n",
    "        \"\"\"Ensure model is loaded before prediction.\"\"\"\n",
    "        if not self._loaded:\n",
    "            print(f\"Loading {self.__class__.__name__}...\")\n",
    "            self.load_model()\n",
    "            self._loaded = True\n",
    "            print(f\"✓ {self.__class__.__name__} ready\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_contours(mask: np.ndarray, min_area: int = 100) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract contours from a binary mask using marching squares.\n",
    "        \n",
    "        Args:\n",
    "            mask: Binary mask (H, W), values 0 or 255\n",
    "            min_area: Minimum contour area to keep\n",
    "        \n",
    "        Returns:\n",
    "            List of contour arrays, each shape (N, 2) as (x, y) coordinates\n",
    "        \"\"\"\n",
    "        # Use skimage for contour extraction (more reliable than cv2 for this use case)\n",
    "        from skimage import measure\n",
    "        \n",
    "        # Ensure binary mask\n",
    "        binary = (mask > 127).astype(np.uint8)\n",
    "        \n",
    "        # Find contours\n",
    "        contours = measure.find_contours(binary, level=0.5)\n",
    "        \n",
    "        result = []\n",
    "        for contour in contours:\n",
    "            # skimage returns (row, col) = (y, x), convert to (x, y)\n",
    "            contour_xy = contour[:, ::-1].astype(np.float32)\n",
    "            \n",
    "            # Filter by area\n",
    "            if len(contour_xy) >= 3:\n",
    "                # Approximate area using shoelace formula\n",
    "                x = contour_xy[:, 0]\n",
    "                y = contour_xy[:, 1]\n",
    "                area = 0.5 * abs(np.sum(x[:-1] * y[1:] - x[1:] * y[:-1]))\n",
    "                if area >= min_area:\n",
    "                    result.append(contour_xy)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "print(\"SegmentationModel base class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ca889",
   "metadata": {},
   "source": [
    "## 3. SAM (Segment Anything Model) Wrapper\n",
    "\n",
    "Meta's SAM enables point-based interactive segmentation. Click anywhere on an object\n",
    "and SAM generates a precise mask for that object.\n",
    "\n",
    "**Model variants:**\n",
    "- `vit_h` — Huge (2.4GB) — Highest quality\n",
    "- `vit_l` — Large (1.2GB) — Good balance\n",
    "- `vit_b` — Base (375MB) — Fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamSegmenter(SegmentationModel):\n",
    "    \"\"\"\n",
    "    Segment Anything Model (SAM) wrapper for interactive point-based segmentation.\n",
    "    \n",
    "    Usage:\n",
    "        sam = SamSegmenter(model_type=\"vit_b\")\n",
    "        result = sam.predict(image, point=(x, y))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model checkpoint URLs\n",
    "    CHECKPOINTS = {\n",
    "        \"vit_h\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
    "        \"vit_l\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
    "        \"vit_b\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_type: str = \"vit_b\", device: str = DEVICE):\n",
    "        super().__init__(device)\n",
    "        self.model_type = model_type\n",
    "        self.predictor = None\n",
    "        self._current_image = None\n",
    "    \n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"Download and load SAM checkpoint.\"\"\"\n",
    "        if not SAM_AVAILABLE:\n",
    "            raise ImportError(\"segment-anything not installed\")\n",
    "        \n",
    "        import urllib.request\n",
    "        \n",
    "        # Download checkpoint if not cached\n",
    "        cache_dir = workspace / \".cache\" / \"sam\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint_url = self.CHECKPOINTS[self.model_type]\n",
    "        checkpoint_name = checkpoint_url.split(\"/\")[-1]\n",
    "        checkpoint_path = cache_dir / checkpoint_name\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"Downloading SAM {self.model_type} checkpoint (~375MB for vit_b)...\")\n",
    "            urllib.request.urlretrieve(checkpoint_url, checkpoint_path)\n",
    "            print(f\"✓ Saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Load model\n",
    "        sam = sam_model_registry[self.model_type](checkpoint=str(checkpoint_path))\n",
    "        sam.to(device=self.device)\n",
    "        self.predictor = SamPredictor(sam)\n",
    "    \n",
    "    def predict(\n",
    "        self, \n",
    "        image: np.ndarray, \n",
    "        point: Optional[Tuple[int, int]] = None\n",
    "    ) -> SegmentationResult:\n",
    "        \"\"\"\n",
    "        Predict segmentation mask for clicked point.\n",
    "        \n",
    "        Args:\n",
    "            image: RGB image (H, W, 3), uint8\n",
    "            point: (x, y) click coordinate — REQUIRED for SAM\n",
    "        \n",
    "        Returns:\n",
    "            SegmentationResult with single mask for clicked object\n",
    "        \"\"\"\n",
    "        self.ensure_loaded()\n",
    "        \n",
    "        if point is None:\n",
    "            # Without a point, return empty result\n",
    "            return SegmentationResult(masks=[], contours=[], labels=[], scores=[])\n",
    "        \n",
    "        # Set image (cached for efficiency)\n",
    "        if self._current_image is not image:\n",
    "            self.predictor.set_image(image)\n",
    "            self._current_image = image\n",
    "        \n",
    "        # Predict with point prompt\n",
    "        input_point = np.array([[point[0], point[1]]])\n",
    "        input_label = np.array([1])  # 1 = foreground\n",
    "        \n",
    "        masks, scores, _ = self.predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        \n",
    "        # Use highest scoring mask\n",
    "        best_idx = np.argmax(scores)\n",
    "        best_mask = (masks[best_idx] * 255).astype(np.uint8)\n",
    "        best_score = float(scores[best_idx])\n",
    "        \n",
    "        # Extract contours\n",
    "        contours = self.extract_contours(best_mask)\n",
    "        \n",
    "        return SegmentationResult(\n",
    "            masks=[best_mask],\n",
    "            contours=contours,\n",
    "            labels=[\"object\"],\n",
    "            scores=[best_score],\n",
    "        )\n",
    "\n",
    "\n",
    "if SAM_AVAILABLE:\n",
    "    print(\"SamSegmenter class defined.\")\n",
    "else:\n",
    "    print(\"⚠ SamSegmenter unavailable (segment-anything not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351aa32b",
   "metadata": {},
   "source": [
    "## 4. YOLO v8 Segmentation Wrapper\n",
    "\n",
    "Ultralytics YOLOv8-seg provides fast automatic instance segmentation for 80 COCO classes.\n",
    "No user interaction required — automatically detects and segments all objects.\n",
    "\n",
    "**Model variants:**\n",
    "- `yolov8n-seg` — Nano (6.7MB) — Fastest\n",
    "- `yolov8s-seg` — Small (22.4MB) — Good balance\n",
    "- `yolov8m-seg` — Medium (50.5MB) — Higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef659d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloSegmenter(SegmentationModel):\n",
    "    \"\"\"\n",
    "    YOLOv8 instance segmentation wrapper for automatic object detection.\n",
    "    \n",
    "    Usage:\n",
    "        yolo = YoloSegmenter(model_name=\"yolov8n-seg\")\n",
    "        result = yolo.predict(image)  # Returns all detected objects\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"yolov8n-seg\", device: str = DEVICE):\n",
    "        super().__init__(device)\n",
    "        self.model_name = model_name\n",
    "        self.conf_threshold = 0.25\n",
    "    \n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"Load YOLOv8 segmentation model (auto-downloads from Ultralytics).\"\"\"\n",
    "        if not YOLO_AVAILABLE:\n",
    "            raise ImportError(\"ultralytics not installed\")\n",
    "        \n",
    "        # YOLO auto-downloads to ~/.cache/ultralytics/\n",
    "        self.model = YOLO(self.model_name)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def predict(\n",
    "        self, \n",
    "        image: np.ndarray, \n",
    "        point: Optional[Tuple[int, int]] = None\n",
    "    ) -> SegmentationResult:\n",
    "        \"\"\"\n",
    "        Predict segmentation masks for all detected objects.\n",
    "        \n",
    "        Args:\n",
    "            image: RGB image (H, W, 3), uint8\n",
    "            point: Optional (x, y) — if provided, only return object containing that point\n",
    "        \n",
    "        Returns:\n",
    "            SegmentationResult with masks for all detected objects (or filtered by point)\n",
    "        \"\"\"\n",
    "        self.ensure_loaded()\n",
    "        \n",
    "        # Run inference\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            results = self.model(image, conf=self.conf_threshold, verbose=False)\n",
    "        \n",
    "        masks = []\n",
    "        contours = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        \n",
    "        if results and len(results) > 0:\n",
    "            result = results[0]\n",
    "            \n",
    "            if result.masks is not None:\n",
    "                h, w = image.shape[:2]\n",
    "                \n",
    "                for i, mask_data in enumerate(result.masks.data):\n",
    "                    # Get mask as numpy array\n",
    "                    mask = mask_data.cpu().numpy()\n",
    "                    \n",
    "                    # Resize to original image size if needed\n",
    "                    if mask.shape != (h, w):\n",
    "                        from PIL import Image as PILImage\n",
    "                        mask_pil = PILImage.fromarray((mask * 255).astype(np.uint8))\n",
    "                        mask_pil = mask_pil.resize((w, h), PILImage.NEAREST)\n",
    "                        mask = np.array(mask_pil)\n",
    "                    else:\n",
    "                        mask = (mask * 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Get class info\n",
    "                    cls_id = int(result.boxes.cls[i])\n",
    "                    conf = float(result.boxes.conf[i])\n",
    "                    label = result.names[cls_id]\n",
    "                    \n",
    "                    # If point specified, check if this mask contains the point\n",
    "                    if point is not None:\n",
    "                        px, py = point\n",
    "                        if 0 <= py < h and 0 <= px < w:\n",
    "                            if mask[py, px] < 128:\n",
    "                                continue  # Skip if point not in this mask\n",
    "                    \n",
    "                    # Extract contours\n",
    "                    mask_contours = self.extract_contours(mask)\n",
    "                    \n",
    "                    masks.append(mask)\n",
    "                    contours.extend(mask_contours)\n",
    "                    labels.append(label)\n",
    "                    scores.append(conf)\n",
    "        \n",
    "        return SegmentationResult(\n",
    "            masks=masks,\n",
    "            contours=contours,\n",
    "            labels=labels,\n",
    "            scores=scores,\n",
    "        )\n",
    "\n",
    "\n",
    "if YOLO_AVAILABLE:\n",
    "    print(\"YoloSegmenter class defined.\")\n",
    "else:\n",
    "    print(\"⚠ YoloSegmenter unavailable (ultralytics not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49538e",
   "metadata": {},
   "source": [
    "## 5. Contour-to-Lines Pipeline (ESD Integration)\n",
    "\n",
    "This section converts segmentation contours to line segments using the LineExtraction framework.\n",
    "\n",
    "**Pipeline:**\n",
    "1. **Contour Simplification** — Douglas-Peucker algorithm to reduce point count\n",
    "2. **ESD Edge Segments** — Use `le_edge.EsdDrawing` on contour image\n",
    "3. **Line Fitting** — Extract line segments with `le_lsd` or direct fitting\n",
    "\n",
    "The key insight: neural segmentation provides accurate object boundaries, while ESD/LSD\n",
    "provides robust line extraction with sub-pixel precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LineExtractionResult:\n",
    "    \"\"\"Result from line extraction pipeline.\"\"\"\n",
    "    contours: List[np.ndarray]  # Original contours (x, y) points\n",
    "    simplified_contours: List[np.ndarray]  # Douglas-Peucker simplified\n",
    "    edge_segments: List[np.ndarray]  # ESD edge segment points\n",
    "    line_segments: List[Tuple[Tuple[float, float], Tuple[float, float]]]  # Line endpoints\n",
    "\n",
    "\n",
    "class ContourToLinesESD:\n",
    "    \"\"\"\n",
    "    Pipeline to convert segmentation contours to line segments using ESD.\n",
    "    \n",
    "    Two approaches are supported:\n",
    "    1. Direct contour simplification (Douglas-Peucker)\n",
    "    2. ESD-based edge segment detection on contour image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        epsilon_ratio: float = 0.01,  # Douglas-Peucker epsilon as ratio of perimeter\n",
    "        min_line_length: float = 10.0,  # Minimum line segment length in pixels\n",
    "        esd_min_pixels: int = 5,  # Minimum pixels for ESD segment\n",
    "    ):\n",
    "        self.epsilon_ratio = epsilon_ratio\n",
    "        self.min_line_length = min_line_length\n",
    "        self.esd_min_pixels = esd_min_pixels\n",
    "    \n",
    "    def simplify_contour(self, contour: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simplify contour using Douglas-Peucker algorithm.\n",
    "        \n",
    "        Args:\n",
    "            contour: Contour points (N, 2) as (x, y)\n",
    "        \n",
    "        Returns:\n",
    "            Simplified contour points\n",
    "        \"\"\"\n",
    "        if len(contour) < 3:\n",
    "            return contour\n",
    "        \n",
    "        # Calculate perimeter\n",
    "        diffs = np.diff(contour, axis=0, append=contour[:1])\n",
    "        perimeter = np.sum(np.linalg.norm(diffs, axis=1))\n",
    "        \n",
    "        # Epsilon as ratio of perimeter\n",
    "        epsilon = self.epsilon_ratio * perimeter\n",
    "        \n",
    "        # Simple recursive Douglas-Peucker\n",
    "        return self._douglas_peucker(contour, epsilon)\n",
    "    \n",
    "    def _douglas_peucker(self, points: np.ndarray, epsilon: float) -> np.ndarray:\n",
    "        \"\"\"Recursive Douglas-Peucker simplification.\"\"\"\n",
    "        if len(points) <= 2:\n",
    "            return points\n",
    "        \n",
    "        # Find point with maximum distance from line between first and last\n",
    "        start, end = points[0], points[-1]\n",
    "        line_vec = end - start\n",
    "        line_len = np.linalg.norm(line_vec)\n",
    "        \n",
    "        if line_len < 1e-10:\n",
    "            return np.array([start, end])\n",
    "        \n",
    "        line_unit = line_vec / line_len\n",
    "        \n",
    "        # Distance from each point to line\n",
    "        dists = np.abs(np.cross(points - start, line_unit))\n",
    "        max_idx = np.argmax(dists)\n",
    "        max_dist = dists[max_idx]\n",
    "        \n",
    "        if max_dist > epsilon:\n",
    "            # Recursively simplify\n",
    "            left = self._douglas_peucker(points[:max_idx + 1], epsilon)\n",
    "            right = self._douglas_peucker(points[max_idx:], epsilon)\n",
    "            return np.vstack([left[:-1], right])\n",
    "        else:\n",
    "            return np.array([start, end])\n",
    "    \n",
    "    def contour_to_edge_image(\n",
    "        self, \n",
    "        contours: List[np.ndarray], \n",
    "        shape: Tuple[int, int]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Render contours as a binary edge image.\n",
    "        \n",
    "        Args:\n",
    "            contours: List of contour arrays (N, 2) as (x, y)\n",
    "            shape: Image shape (H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Binary edge image (H, W), uint8\n",
    "        \"\"\"\n",
    "        edge_img = np.zeros(shape, dtype=np.uint8)\n",
    "        \n",
    "        for contour in contours:\n",
    "            pts = contour.astype(np.int32)\n",
    "            for i in range(len(pts)):\n",
    "                p1 = pts[i]\n",
    "                p2 = pts[(i + 1) % len(pts)]\n",
    "                # Draw line using Bresenham\n",
    "                self._draw_line(edge_img, p1, p2)\n",
    "        \n",
    "        return edge_img\n",
    "    \n",
    "    def _draw_line(self, img: np.ndarray, p1: np.ndarray, p2: np.ndarray) -> None:\n",
    "        \"\"\"Draw line on image using Bresenham's algorithm.\"\"\"\n",
    "        x0, y0 = p1\n",
    "        x1, y1 = p2\n",
    "        h, w = img.shape\n",
    "        \n",
    "        dx = abs(x1 - x0)\n",
    "        dy = -abs(y1 - y0)\n",
    "        sx = 1 if x0 < x1 else -1\n",
    "        sy = 1 if y0 < y1 else -1\n",
    "        err = dx + dy\n",
    "        \n",
    "        while True:\n",
    "            if 0 <= x0 < w and 0 <= y0 < h:\n",
    "                img[y0, x0] = 255\n",
    "            \n",
    "            if x0 == x1 and y0 == y1:\n",
    "                break\n",
    "            \n",
    "            e2 = 2 * err\n",
    "            if e2 >= dy:\n",
    "                err += dy\n",
    "                x0 += sx\n",
    "            if e2 <= dx:\n",
    "                err += dx\n",
    "                y0 += sy\n",
    "    \n",
    "    def extract_lines_from_contours(\n",
    "        self, \n",
    "        contours: List[np.ndarray],\n",
    "        image_shape: Tuple[int, int]\n",
    "    ) -> LineExtractionResult:\n",
    "        \"\"\"\n",
    "        Extract line segments from contours using ESD.\n",
    "        \n",
    "        Args:\n",
    "            contours: List of contour arrays from segmentation\n",
    "            image_shape: Original image shape (H, W)\n",
    "        \n",
    "        Returns:\n",
    "            LineExtractionResult with all intermediate and final results\n",
    "        \"\"\"\n",
    "        if not contours:\n",
    "            return LineExtractionResult(\n",
    "                contours=[],\n",
    "                simplified_contours=[],\n",
    "                edge_segments=[],\n",
    "                line_segments=[],\n",
    "            )\n",
    "        \n",
    "        # 1. Simplify contours using Douglas-Peucker\n",
    "        simplified = [self.simplify_contour(c) for c in contours]\n",
    "        \n",
    "        # 2. Create edge image from contours\n",
    "        edge_img = self.contour_to_edge_image(contours, image_shape)\n",
    "        \n",
    "        # 3. Use EdgeSource on the edge image to get gradients\n",
    "        edge_source = le_imgproc.SobelGradient()\n",
    "        edge_source.process(edge_img)\n",
    "        \n",
    "        # 4. Run ESD on the edge image\n",
    "        try:\n",
    "            esd = le_edge.EsdDrawing(min_pixels=self.esd_min_pixels)\n",
    "            \n",
    "            # For ESD we need: direction_map, magnitude, seeds\n",
    "            # Create a simple NMS to get seeds\n",
    "            nms = le_edge.NmsPatternFull()\n",
    "            nms.process(edge_source)\n",
    "            \n",
    "            # Get NMS results\n",
    "            nms_img = nms.img_nms()\n",
    "            \n",
    "            # Detect edge segments\n",
    "            esd.detect(nms, edge_source)\n",
    "            \n",
    "            # Get segment points\n",
    "            points = esd.points()\n",
    "            segments = esd.segments()\n",
    "            \n",
    "            edge_segment_list = []\n",
    "            for seg in segments:\n",
    "                # Extract points for this segment\n",
    "                seg_points = []\n",
    "                for idx in range(seg.begin, seg.end):\n",
    "                    if idx < len(points):\n",
    "                        pt = points[idx]\n",
    "                        # Convert linear index to (x, y)\n",
    "                        y = pt // image_shape[1]\n",
    "                        x = pt % image_shape[1]\n",
    "                        seg_points.append([x, y])\n",
    "                if seg_points:\n",
    "                    edge_segment_list.append(np.array(seg_points))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ESD processing failed: {e}\")\n",
    "            edge_segment_list = []\n",
    "        \n",
    "        # 5. Extract line segments from simplified contours\n",
    "        line_segments = []\n",
    "        for simp in simplified:\n",
    "            for i in range(len(simp) - 1):\n",
    "                p1 = tuple(simp[i])\n",
    "                p2 = tuple(simp[i + 1])\n",
    "                \n",
    "                # Filter by length\n",
    "                length = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "                if length >= self.min_line_length:\n",
    "                    line_segments.append((p1, p2))\n",
    "            \n",
    "            # Close the contour if needed\n",
    "            if len(simp) >= 3:\n",
    "                p1 = tuple(simp[-1])\n",
    "                p2 = tuple(simp[0])\n",
    "                length = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "                if length >= self.min_line_length:\n",
    "                    line_segments.append((p1, p2))\n",
    "        \n",
    "        return LineExtractionResult(\n",
    "            contours=contours,\n",
    "            simplified_contours=simplified,\n",
    "            edge_segments=edge_segment_list,\n",
    "            line_segments=line_segments,\n",
    "        )\n",
    "\n",
    "\n",
    "# Instantiate pipeline\n",
    "contour_pipeline = ContourToLinesESD()\n",
    "print(\"ContourToLinesESD pipeline defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d0415",
   "metadata": {},
   "source": [
    "## 6. Visualization Utilities\n",
    "\n",
    "Helper functions for displaying results at each stage of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_contours_on_image(\n",
    "    image: np.ndarray,\n",
    "    contours: List[np.ndarray],\n",
    "    color: Tuple[int, int, int] = (0, 255, 0),\n",
    "    thickness: int = 2,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Draw contours on image copy.\"\"\"\n",
    "    result = image.copy()\n",
    "    \n",
    "    for contour in contours:\n",
    "        pts = contour.astype(np.int32)\n",
    "        for i in range(len(pts)):\n",
    "            p1 = tuple(pts[i])\n",
    "            p2 = tuple(pts[(i + 1) % len(pts)])\n",
    "            # Simple line drawing\n",
    "            draw_line_on_image(result, p1, p2, color, thickness)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def draw_line_on_image(\n",
    "    img: np.ndarray,\n",
    "    p1: Tuple[int, int],\n",
    "    p2: Tuple[int, int],\n",
    "    color: Tuple[int, int, int],\n",
    "    thickness: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"Draw a line on image using simple algorithm.\"\"\"\n",
    "    x0, y0 = int(p1[0]), int(p1[1])\n",
    "    x1, y1 = int(p2[0]), int(p2[1])\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Bresenham with thickness\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = -abs(y1 - y0)\n",
    "    sx = 1 if x0 < x1 else -1\n",
    "    sy = 1 if y0 < y1 else -1\n",
    "    err = dx + dy\n",
    "    \n",
    "    while True:\n",
    "        for tx in range(-thickness // 2, thickness // 2 + 1):\n",
    "            for ty in range(-thickness // 2, thickness // 2 + 1):\n",
    "                px, py = x0 + tx, y0 + ty\n",
    "                if 0 <= px < w and 0 <= py < h:\n",
    "                    img[py, px] = color\n",
    "        \n",
    "        if x0 == x1 and y0 == y1:\n",
    "            break\n",
    "        \n",
    "        e2 = 2 * err\n",
    "        if e2 >= dy:\n",
    "            err += dy\n",
    "            x0 += sx\n",
    "        if e2 <= dx:\n",
    "            err += dx\n",
    "            y0 += sy\n",
    "\n",
    "\n",
    "def draw_lines_on_image(\n",
    "    image: np.ndarray,\n",
    "    lines: List[Tuple[Tuple[float, float], Tuple[float, float]]],\n",
    "    color: Tuple[int, int, int] = (255, 0, 0),\n",
    "    thickness: int = 2,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Draw line segments on image copy.\"\"\"\n",
    "    result = image.copy()\n",
    "    \n",
    "    for p1, p2 in lines:\n",
    "        draw_line_on_image(result, (int(p1[0]), int(p1[1])), (int(p2[0]), int(p2[1])), color, thickness)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def overlay_mask(\n",
    "    image: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    color: Tuple[int, int, int] = (0, 120, 255),\n",
    "    alpha: float = 0.4,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Overlay a binary mask on image with transparency.\"\"\"\n",
    "    result = image.copy()\n",
    "    \n",
    "    # Create colored overlay\n",
    "    overlay = np.zeros_like(result)\n",
    "    mask_bool = mask > 127\n",
    "    overlay[mask_bool] = color\n",
    "    \n",
    "    # Blend\n",
    "    result[mask_bool] = (\n",
    "        (1 - alpha) * result[mask_bool] + alpha * overlay[mask_bool]\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def visualize_pipeline_result(\n",
    "    image: np.ndarray,\n",
    "    seg_result: SegmentationResult,\n",
    "    line_result: LineExtractionResult,\n",
    "    figsize: Tuple[int, int] = (16, 4),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the complete pipeline: Original → Segmentation → Contours → Lines\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=figsize)\n",
    "    \n",
    "    # 1. Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # 2. Segmentation mask overlay\n",
    "    if seg_result.masks:\n",
    "        masked = image.copy()\n",
    "        colors = [(255, 100, 100), (100, 255, 100), (100, 100, 255), (255, 255, 100)]\n",
    "        for i, mask in enumerate(seg_result.masks):\n",
    "            color = colors[i % len(colors)]\n",
    "            masked = overlay_mask(masked, mask, color, alpha=0.4)\n",
    "        axes[1].imshow(masked)\n",
    "        title = \"Segmentation\"\n",
    "        if seg_result.labels:\n",
    "            title += f\"\\n{', '.join(seg_result.labels[:3])}\"\n",
    "    else:\n",
    "        axes[1].imshow(image)\n",
    "        title = \"No Segmentation\"\n",
    "    axes[1].set_title(title)\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    # 3. Contours\n",
    "    if line_result.contours:\n",
    "        contour_img = draw_contours_on_image(image, line_result.contours, (0, 255, 0), 2)\n",
    "        axes[2].imshow(contour_img)\n",
    "        axes[2].set_title(f\"Contours ({len(line_result.contours)})\")\n",
    "    else:\n",
    "        axes[2].imshow(image)\n",
    "        axes[2].set_title(\"No Contours\")\n",
    "    axes[2].axis(\"off\")\n",
    "    \n",
    "    # 4. Line segments\n",
    "    if line_result.line_segments:\n",
    "        line_img = draw_lines_on_image(image, line_result.line_segments, (255, 0, 0), 2)\n",
    "        axes[3].imshow(line_img)\n",
    "        axes[3].set_title(f\"Lines ({len(line_result.line_segments)})\")\n",
    "    else:\n",
    "        axes[3].imshow(image)\n",
    "        axes[3].set_title(\"No Lines\")\n",
    "    axes[3].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Visualization utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a93915",
   "metadata": {},
   "source": [
    "## 7. Interactive Demo Application\n",
    "\n",
    "The interactive application provides:\n",
    "- **Image selection** from BSDS500/MDB datasets\n",
    "- **Model switching** between SAM and YOLO\n",
    "- **Click-to-segment** for SAM (click on any object)\n",
    "- **Auto-detect** for YOLO (detects all objects)\n",
    "- **Real-time line extraction** on segmented objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85861170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveSegmentationDemo:\n",
    "    \"\"\"\n",
    "    Interactive demo combining PyTorch segmentation with ESD line extraction.\n",
    "    \n",
    "    Features:\n",
    "    - Switch between SAM and YOLO models\n",
    "    - Click on image to segment (SAM mode)\n",
    "    - Automatic segmentation (YOLO mode)\n",
    "    - Real-time line extraction from contours\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_images = TestImages()\n",
    "        self.current_image: Optional[np.ndarray] = None\n",
    "        self.current_path: Optional[pathlib.Path] = None\n",
    "        \n",
    "        # Models (lazy-loaded)\n",
    "        self._sam: Optional[SamSegmenter] = None\n",
    "        self._yolo: Optional[YoloSegmenter] = None\n",
    "        self.current_model = \"yolo\"  # Default to YOLO (no click required)\n",
    "        \n",
    "        # Results\n",
    "        self.seg_result: Optional[SegmentationResult] = None\n",
    "        self.line_result: Optional[LineExtractionResult] = None\n",
    "        \n",
    "        # Pipeline\n",
    "        self.pipeline = ContourToLinesESD()\n",
    "        \n",
    "        # UI elements\n",
    "        self.fig = None\n",
    "        self.ax = None\n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "    @property\n",
    "    def sam(self) -> SamSegmenter:\n",
    "        if self._sam is None:\n",
    "            if not SAM_AVAILABLE:\n",
    "                raise ImportError(\"SAM not available\")\n",
    "            self._sam = SamSegmenter()\n",
    "        return self._sam\n",
    "    \n",
    "    @property\n",
    "    def yolo(self) -> YoloSegmenter:\n",
    "        if self._yolo is None:\n",
    "            if not YOLO_AVAILABLE:\n",
    "                raise ImportError(\"YOLO not available\")\n",
    "            self._yolo = YoloSegmenter()\n",
    "        return self._yolo\n",
    "    \n",
    "    def load_image(self, path: Union[str, pathlib.Path]) -> np.ndarray:\n",
    "        \"\"\"Load image from path.\"\"\"\n",
    "        path = pathlib.Path(path)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        self.current_image = np.array(img)\n",
    "        self.current_path = path\n",
    "        \n",
    "        # Reset results\n",
    "        self.seg_result = None\n",
    "        self.line_result = None\n",
    "        \n",
    "        return self.current_image\n",
    "    \n",
    "    def get_available_images(self, dataset: str = \"bsds500\", limit: int = 20) -> List[pathlib.Path]:\n",
    "        \"\"\"Get list of available images from dataset.\"\"\"\n",
    "        if dataset == \"bsds500\":\n",
    "            return list(self.test_images.bsds500())[:limit]\n",
    "        elif dataset == \"mdb\":\n",
    "            scenes = list(self.test_images.stereo_scenes(\"Q\"))[:limit]\n",
    "            return [self.test_images.stereo_pair(s, \"Q\")[0] for s in scenes]\n",
    "        elif dataset == \"noise\":\n",
    "            return list(self.test_images.noise_images())\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def segment(self, point: Optional[Tuple[int, int]] = None) -> SegmentationResult:\n",
    "        \"\"\"Run segmentation on current image.\"\"\"\n",
    "        if self.current_image is None:\n",
    "            raise ValueError(\"No image loaded\")\n",
    "        \n",
    "        if self.current_model == \"sam\":\n",
    "            if point is None:\n",
    "                return SegmentationResult(masks=[], contours=[], labels=[], scores=[])\n",
    "            self.seg_result = self.sam.predict(self.current_image, point)\n",
    "        else:  # yolo\n",
    "            self.seg_result = self.yolo.predict(self.current_image, point)\n",
    "        \n",
    "        return self.seg_result\n",
    "    \n",
    "    def extract_lines(self) -> LineExtractionResult:\n",
    "        \"\"\"Extract lines from segmentation result.\"\"\"\n",
    "        if self.seg_result is None or not self.seg_result.contours:\n",
    "            self.line_result = LineExtractionResult(\n",
    "                contours=[], simplified_contours=[], edge_segments=[], line_segments=[]\n",
    "            )\n",
    "        else:\n",
    "            h, w = self.current_image.shape[:2]\n",
    "            self.line_result = self.pipeline.extract_lines_from_contours(\n",
    "                self.seg_result.contours, (h, w)\n",
    "            )\n",
    "        \n",
    "        return self.line_result\n",
    "    \n",
    "    def run_pipeline(self, point: Optional[Tuple[int, int]] = None) -> None:\n",
    "        \"\"\"Run full pipeline and display results.\"\"\"\n",
    "        self.segment(point)\n",
    "        self.extract_lines()\n",
    "        \n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            if self.current_image is not None and self.seg_result is not None:\n",
    "                visualize_pipeline_result(\n",
    "                    self.current_image,\n",
    "                    self.seg_result,\n",
    "                    self.line_result,\n",
    "                )\n",
    "    \n",
    "    def on_click(self, event):\n",
    "        \"\"\"Handle mouse click on image.\"\"\"\n",
    "        if event.inaxes != self.ax:\n",
    "            return\n",
    "        \n",
    "        x, y = int(event.xdata), int(event.ydata)\n",
    "        print(f\"Click at ({x}, {y})\")\n",
    "        \n",
    "        self.run_pipeline(point=(x, y))\n",
    "    \n",
    "    def create_ui(self) -> widgets.Widget:\n",
    "        \"\"\"Create the interactive UI.\"\"\"\n",
    "        # Dataset selector\n",
    "        dataset_dropdown = widgets.Dropdown(\n",
    "            options=[\"bsds500\", \"mdb\", \"noise\"],\n",
    "            value=\"bsds500\",\n",
    "            description=\"Dataset:\",\n",
    "        )\n",
    "        \n",
    "        # Image selector (populated dynamically)\n",
    "        image_dropdown = widgets.Dropdown(\n",
    "            options=[],\n",
    "            description=\"Image:\",\n",
    "        )\n",
    "        \n",
    "        # Model selector\n",
    "        model_toggle = widgets.ToggleButtons(\n",
    "            options=[\"YOLO (Auto)\", \"SAM (Click)\"],\n",
    "            value=\"YOLO (Auto)\",\n",
    "            description=\"Model:\",\n",
    "        )\n",
    "        \n",
    "        # Buttons\n",
    "        load_btn = widgets.Button(description=\"Load Image\", button_style=\"primary\")\n",
    "        detect_btn = widgets.Button(description=\"Detect Objects\", button_style=\"success\")\n",
    "        \n",
    "        # Status\n",
    "        status = widgets.HTML(value=\"<i>Select an image to begin</i>\")\n",
    "        \n",
    "        def update_images(change):\n",
    "            paths = self.get_available_images(change[\"new\"])\n",
    "            image_dropdown.options = [(p.name, p) for p in paths]\n",
    "        \n",
    "        def on_load(btn):\n",
    "            path = image_dropdown.value\n",
    "            if path:\n",
    "                self.load_image(path)\n",
    "                status.value = f\"<b>Loaded:</b> {path.name} ({self.current_image.shape[1]}x{self.current_image.shape[0]})\"\n",
    "                \n",
    "                # Show image\n",
    "                with self.output:\n",
    "                    clear_output(wait=True)\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    plt.imshow(self.current_image)\n",
    "                    plt.title(f\"{path.name} - Click to segment (SAM) or press Detect (YOLO)\")\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.show()\n",
    "        \n",
    "        def on_detect(btn):\n",
    "            if self.current_image is None:\n",
    "                status.value = \"<span style='color:red'>Load an image first!</span>\"\n",
    "                return\n",
    "            \n",
    "            status.value = \"<i>Processing...</i>\"\n",
    "            self.run_pipeline()\n",
    "            status.value = f\"<b>Done:</b> {len(self.seg_result.masks)} objects, {len(self.line_result.line_segments)} lines\"\n",
    "        \n",
    "        def on_model_change(change):\n",
    "            if \"Auto\" in change[\"new\"]:\n",
    "                self.current_model = \"yolo\"\n",
    "            else:\n",
    "                self.current_model = \"sam\"\n",
    "        \n",
    "        # Connect callbacks\n",
    "        dataset_dropdown.observe(update_images, names=\"value\")\n",
    "        load_btn.on_click(on_load)\n",
    "        detect_btn.on_click(on_detect)\n",
    "        model_toggle.observe(on_model_change, names=\"value\")\n",
    "        \n",
    "        # Initialize image list\n",
    "        update_images({\"new\": \"bsds500\"})\n",
    "        \n",
    "        # Layout\n",
    "        controls = widgets.VBox([\n",
    "            widgets.HBox([dataset_dropdown, image_dropdown, load_btn]),\n",
    "            widgets.HBox([model_toggle, detect_btn]),\n",
    "            status,\n",
    "        ])\n",
    "        \n",
    "        return widgets.VBox([controls, self.output])\n",
    "\n",
    "\n",
    "# Create demo instance\n",
    "demo = InteractiveSegmentationDemo()\n",
    "print(\"InteractiveSegmentationDemo created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97f57b",
   "metadata": {},
   "source": [
    "### Run the Interactive Demo\n",
    "\n",
    "Execute the cell below to launch the interactive UI. \n",
    "\n",
    "**How to use:**\n",
    "1. Select a dataset (BSDS500 recommended for diverse images)\n",
    "2. Choose an image from the dropdown\n",
    "3. Click **Load Image** to display it\n",
    "4. Choose model: **YOLO** for automatic detection, **SAM** for click-to-segment\n",
    "5. Click **Detect Objects** to run the pipeline\n",
    "6. View results: Original → Segmentation → Contours → Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the interactive demo\n",
    "ui = demo.create_ui()\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94319a7",
   "metadata": {},
   "source": [
    "## 8. Quick Demo (Non-Interactive)\n",
    "\n",
    "For testing without the interactive UI, run this cell to process a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974721b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo: Load a test image and run the full pipeline\n",
    "\n",
    "# Get first available image\n",
    "test_images = TestImages()\n",
    "available = list(test_images.bsds500())\n",
    "\n",
    "if available:\n",
    "    # Load image\n",
    "    img_path = available[0]\n",
    "    print(f\"Loading: {img_path.name}\")\n",
    "    \n",
    "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    print(f\"Image shape: {image.shape}\")\n",
    "    \n",
    "    # Run YOLO segmentation (if available)\n",
    "    if YOLO_AVAILABLE:\n",
    "        print(\"\\nRunning YOLO segmentation...\")\n",
    "        yolo = YoloSegmenter()\n",
    "        seg_result = yolo.predict(image)\n",
    "        print(f\"Detected {len(seg_result.masks)} objects: {seg_result.labels}\")\n",
    "        \n",
    "        # Extract lines\n",
    "        print(\"\\nExtracting lines...\")\n",
    "        pipeline = ContourToLinesESD()\n",
    "        line_result = pipeline.extract_lines_from_contours(\n",
    "            seg_result.contours, \n",
    "            image.shape[:2]\n",
    "        )\n",
    "        print(f\"Extracted {len(line_result.line_segments)} line segments\")\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_pipeline_result(image, seg_result, line_result)\n",
    "    elif SAM_AVAILABLE:\n",
    "        print(\"\\nYOLO not available, using SAM with center point...\")\n",
    "        h, w = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        \n",
    "        sam = SamSegmenter()\n",
    "        seg_result = sam.predict(image, point=center)\n",
    "        print(f\"Segmented object at center: score={seg_result.scores[0]:.2f}\")\n",
    "        \n",
    "        # Extract lines\n",
    "        pipeline = ContourToLinesESD()\n",
    "        line_result = pipeline.extract_lines_from_contours(\n",
    "            seg_result.contours,\n",
    "            image.shape[:2]\n",
    "        )\n",
    "        print(f\"Extracted {len(line_result.line_segments)} line segments\")\n",
    "        \n",
    "        visualize_pipeline_result(image, seg_result, line_result)\n",
    "    else:\n",
    "        print(\"⚠ Neither YOLO nor SAM available. Install: uv pip install ultralytics segment-anything torch\")\n",
    "else:\n",
    "    print(\"⚠ No test images found. Ensure BSDS500 dataset is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792eee6f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the integration of PyTorch-based object segmentation with the\n",
    "LineExtraction ESD framework:\n",
    "\n",
    "### Pipeline Components\n",
    "\n",
    "| Stage | Component | Description |\n",
    "|-------|-----------|-------------|\n",
    "| **Segmentation** | SAM / YOLO | Neural network generates pixel-accurate object masks |\n",
    "| **Contour Extraction** | skimage | Marching squares extracts polygon contours from masks |\n",
    "| **Simplification** | Douglas-Peucker | Reduces contour points while preserving shape |\n",
    "| **Line Extraction** | ESD + LSD | LineExtraction framework converts contours to line segments |\n",
    "\n",
    "### Key Classes\n",
    "\n",
    "- `SegmentationModel` — Abstract base for segmentation models\n",
    "- `SamSegmenter` — Segment Anything Model wrapper (click-to-segment)\n",
    "- `YoloSegmenter` — YOLOv8 instance segmentation (automatic)\n",
    "- `ContourToLinesESD` — Pipeline connecting segmentation to LineExtraction\n",
    "\n",
    "### Extensions\n",
    "\n",
    "1. **Add more models:** Implement `SegmentationModel` for other architectures (DeepLabV3, Mask R-CNN)\n",
    "2. **Improve line fitting:** Use `le_lsd` detectors on edge images for sub-pixel accuracy\n",
    "3. **Batch processing:** Process multiple images and aggregate statistics\n",
    "4. **Export results:** Save line segments as SVG or other vector formats\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "```bash\n",
    "# Core\n",
    "bazel build //libs/...  # Build LE Python bindings\n",
    "\n",
    "# PyTorch ecosystem\n",
    "uv pip install torch torchvision segment-anything ultralytics\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
